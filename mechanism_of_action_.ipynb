{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGrJG14W5HXS"
   },
   "source": [
    "# Mechanisms of Action (MoA) Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzBcDe1ohzwK"
   },
   "source": [
    "How do we determine the MoAs of a new drug?\n",
    "\n",
    "One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.\n",
    "\n",
    "In this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cellsâ€™ responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset.\n",
    "\n",
    "As is customary, the dataset has been split into testing and training subsets. Hence, your task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.\n",
    "\n",
    "Based on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjxvH6EFAJTv"
   },
   "source": [
    "In this competition, you will be predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data.\n",
    "\n",
    "train_features.csv - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n",
    "\n",
    "train_drug.csv - This file contains an anonymous drug_id for the training set only.\n",
    "\n",
    "train_targets_scored.csv - The binary MoA targets that are scored.\n",
    "\n",
    "train_targets_nonscored.csv - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\n",
    "\n",
    "test_features.csv - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.\n",
    "\n",
    "sample_submission.csv - A submission file in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XOM_WouUhzOi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import os\n",
    "import time\n",
    "import plotly.express as px\n",
    "import random\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LmzzOQ-qLKv3"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huUcVegMh437",
    "outputId": "7835eebb-e504-443a-ac81-8ea0caf92956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading lish-moa.zip to /content\n",
      "100% 64.7M/64.7M [00:02<00:00, 36.9MB/s]\n",
      "100% 64.7M/64.7M [00:02<00:00, 25.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c lish-moa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ih5aSHFnh7C7",
    "outputId": "79c45843-36c0-4528-8e69-fcbf5798053c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/lish-moa.zip\n",
      "  inflating: sample_submission.csv   \n",
      "  inflating: test_features.csv       \n",
      "  inflating: train_drug.csv          \n",
      "  inflating: train_features.csv      \n",
      "  inflating: train_targets_nonscored.csv  \n",
      "  inflating: train_targets_scored.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip \"/content/lish-moa.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2RDo3yNxjfU"
   },
   "source": [
    "train_features.csv - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "5pe5snzgiAEC",
    "outputId": "51987b04-9eeb-4e5c-db36-c4699da8e255"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-83183ab6-e481-4e35-b8bb-cf2f28d16bea\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>ctl_vehicle</td>\n",
       "      <td>48</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows Ã— 876 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83183ab6-e481-4e35-b8bb-cf2f28d16bea')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-83183ab6-e481-4e35-b8bb-cf2f28d16bea button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-83183ab6-e481-4e35-b8bb-cf2f28d16bea');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             sig_id      cp_type  cp_time cp_dose     g-0     g-1     g-2  \\\n",
       "0      id_000644bb2       trt_cp       24      D1  1.0620  0.5577 -0.2479   \n",
       "1      id_000779bfc       trt_cp       72      D1  0.0743  0.4087  0.2991   \n",
       "2      id_000a6266a       trt_cp       48      D1  0.6280  0.5817  1.5540   \n",
       "3      id_0015fd391       trt_cp       48      D1 -0.5138 -0.2491 -0.2656   \n",
       "4      id_001626bd3       trt_cp       72      D2 -0.3254 -0.4009  0.9700   \n",
       "...             ...          ...      ...     ...     ...     ...     ...   \n",
       "23809  id_fffb1ceed       trt_cp       24      D2  0.1394 -0.0636 -0.1112   \n",
       "23810  id_fffb70c0c       trt_cp       24      D2 -1.3260  0.3478 -0.3743   \n",
       "23811  id_fffc1c3f4  ctl_vehicle       48      D2  0.3942  0.3756  0.3109   \n",
       "23812  id_fffcb9e7c       trt_cp       24      D1  0.6660  0.2324  0.4392   \n",
       "23813  id_ffffdd77b       trt_cp       72      D1 -0.8598  1.0240 -0.1361   \n",
       "\n",
       "          g-3     g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94  \\\n",
       "0     -0.6208 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912   \n",
       "1      0.0604  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957   \n",
       "2     -0.0764 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240   \n",
       "3      0.5288  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632   \n",
       "4      0.6919  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523   \n",
       "...       ...     ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "23809 -0.5080 -0.4713  0.7201  ...  0.1969  0.0262 -0.8121  0.3434  0.5372   \n",
       "23810  0.9905 -0.7178  0.6621  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086   \n",
       "23811 -0.7389  0.5505 -0.0159  ...  0.5409  0.3755  0.7343  0.2807  0.4116   \n",
       "23812  0.2044  0.8531 -0.0343  ... -0.1105  0.4258 -0.2012  0.1506  1.5230   \n",
       "23813  0.7952 -0.3611 -3.6750  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860   \n",
       "\n",
       "         c-95    c-96    c-97    c-98    c-99  \n",
       "0      0.6584 -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.4899  0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.3174 -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.2880 -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4     -0.3031  0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...     ...  \n",
       "23809 -0.3246  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.9798 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.6422  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.7101  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -1.4160 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[23814 rows x 876 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features=pd.read_csv(\"/content/train_features.csv\")\n",
    "train_features\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "NoKPizNviDvS",
    "outputId": "d71582fb-8dd6-4f18-d51f-8dd9a71b14cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-10e66adf-2676-44d3-9662-a80ac33f3fa3\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows Ã— 207 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10e66adf-2676-44d3-9662-a80ac33f3fa3')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-10e66adf-2676-44d3-9662-a80ac33f3fa3 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-10e66adf-2676-44d3-9662-a80ac33f3fa3');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0      id_000644bb2                            0                       0   \n",
       "1      id_000779bfc                            0                       0   \n",
       "2      id_000a6266a                            0                       0   \n",
       "3      id_0015fd391                            0                       0   \n",
       "4      id_001626bd3                            0                       0   \n",
       "...             ...                          ...                     ...   \n",
       "23809  id_fffb1ceed                            0                       0   \n",
       "23810  id_fffb70c0c                            0                       0   \n",
       "23811  id_fffc1c3f4                            0                       0   \n",
       "23812  id_fffcb9e7c                            0                       0   \n",
       "23813  id_ffffdd77b                            0                       0   \n",
       "\n",
       "       acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0                   0                               0   \n",
       "1                   0                               0   \n",
       "2                   0                               0   \n",
       "3                   0                               0   \n",
       "4                   0                               0   \n",
       "...               ...                             ...   \n",
       "23809               0                               0   \n",
       "23810               0                               0   \n",
       "23811               0                               0   \n",
       "23812               0                               0   \n",
       "23813               0                               0   \n",
       "\n",
       "       acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                                      0                               0   \n",
       "1                                      0                               0   \n",
       "2                                      0                               0   \n",
       "3                                      0                               0   \n",
       "4                                      0                               0   \n",
       "...                                  ...                             ...   \n",
       "23809                                  0                               0   \n",
       "23810                                  0                               0   \n",
       "23811                                  0                               0   \n",
       "23812                                  0                               0   \n",
       "23813                                  0                               0   \n",
       "\n",
       "       adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                               0                              0   \n",
       "1                               0                              0   \n",
       "2                               0                              0   \n",
       "3                               0                              0   \n",
       "4                               0                              0   \n",
       "...                           ...                            ...   \n",
       "23809                           0                              0   \n",
       "23810                           0                              0   \n",
       "23811                           0                              0   \n",
       "23812                           0                              0   \n",
       "23813                           0                              0   \n",
       "\n",
       "       adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                               0  ...                                      0   \n",
       "1                               0  ...                                      0   \n",
       "2                               0  ...                                      0   \n",
       "3                               0  ...                                      0   \n",
       "4                               0  ...                                      0   \n",
       "...                           ...  ...                                    ...   \n",
       "23809                           0  ...                                      0   \n",
       "23810                           0  ...                                      0   \n",
       "23811                           0  ...                                      0   \n",
       "23812                           0  ...                                      0   \n",
       "23813                           0  ...                                      0   \n",
       "\n",
       "       trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0                 0                0                  0   \n",
       "1                 0                0                  0   \n",
       "2                 0                0                  0   \n",
       "3                 0                0                  0   \n",
       "4                 0                0                  0   \n",
       "...             ...              ...                ...   \n",
       "23809             0                0                  0   \n",
       "23810             0                0                  0   \n",
       "23811             0                0                  0   \n",
       "23812             0                0                  0   \n",
       "23813             0                0                  0   \n",
       "\n",
       "       tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                              0                                      0   \n",
       "1                              0                                      0   \n",
       "2                              0                                      0   \n",
       "3                              0                                      0   \n",
       "4                              0                                      0   \n",
       "...                          ...                                    ...   \n",
       "23809                          0                                      0   \n",
       "23810                          0                                      0   \n",
       "23811                          0                                      0   \n",
       "23812                          0                                      0   \n",
       "23813                          0                                      0   \n",
       "\n",
       "       vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                    0          0                           0              0  \n",
       "1                    0          0                           0              0  \n",
       "2                    0          0                           0              0  \n",
       "3                    0          0                           0              0  \n",
       "4                    0          0                           0              0  \n",
       "...                ...        ...                         ...            ...  \n",
       "23809                0          0                           0              0  \n",
       "23810                0          0                           0              0  \n",
       "23811                0          0                           0              0  \n",
       "23812                0          0                           0              0  \n",
       "23813                0          0                           0              0  \n",
       "\n",
       "[23814 rows x 207 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets_scored=pd.read_csv(\"/content/train_targets_scored.csv\")\n",
    "train_targets_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "Jpiesw52iM16",
    "outputId": "143d29c9-8c54-4477-b499-738dc746b96a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-05ed2ad3-a846-4882-8a34-3ac81e84ae83\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5458</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>-0.5135</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-0.1644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>-0.1430</td>\n",
       "      <td>-0.2067</td>\n",
       "      <td>-0.2303</td>\n",
       "      <td>-0.1193</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>-0.7750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.1829</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>1.2080</td>\n",
       "      <td>-0.4522</td>\n",
       "      <td>-0.3652</td>\n",
       "      <td>-0.3319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>-0.1852</td>\n",
       "      <td>-1.0310</td>\n",
       "      <td>-1.3670</td>\n",
       "      <td>-0.3690</td>\n",
       "      <td>-0.5382</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>-0.4764</td>\n",
       "      <td>-1.3810</td>\n",
       "      <td>-0.7300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>ctl_vehicle</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>-0.1404</td>\n",
       "      <td>-0.3911</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>-1.4380</td>\n",
       "      <td>0.2455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2261</td>\n",
       "      <td>0.3370</td>\n",
       "      <td>-1.3840</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>-1.9530</td>\n",
       "      <td>-1.0140</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>-0.1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.4244</td>\n",
       "      <td>-0.5855</td>\n",
       "      <td>-1.2020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>-0.1784</td>\n",
       "      <td>-1.1200</td>\n",
       "      <td>-0.4325</td>\n",
       "      <td>-0.9005</td>\n",
       "      <td>0.8131</td>\n",
       "      <td>-0.1305</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>-0.5809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.3979</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>1.9130</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>-0.5864</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4965</td>\n",
       "      <td>0.7578</td>\n",
       "      <td>-0.1580</td>\n",
       "      <td>1.0510</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>1.0900</td>\n",
       "      <td>-0.2962</td>\n",
       "      <td>-0.5313</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>1.8380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>id_ff7004b87</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>-0.5743</td>\n",
       "      <td>3.3930</td>\n",
       "      <td>-0.6202</td>\n",
       "      <td>0.8557</td>\n",
       "      <td>1.6240</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1790</td>\n",
       "      <td>-0.6422</td>\n",
       "      <td>-0.4367</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>-0.6539</td>\n",
       "      <td>-0.4791</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>-1.1280</td>\n",
       "      <td>-0.4167</td>\n",
       "      <td>-0.6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>id_ff925dd0d</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5885</td>\n",
       "      <td>-0.2548</td>\n",
       "      <td>2.5850</td>\n",
       "      <td>0.3456</td>\n",
       "      <td>0.4401</td>\n",
       "      <td>0.3107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>-0.5888</td>\n",
       "      <td>0.8057</td>\n",
       "      <td>0.9312</td>\n",
       "      <td>1.2730</td>\n",
       "      <td>0.2614</td>\n",
       "      <td>-0.2790</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>-0.0934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>id_ffb710450</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.3985</td>\n",
       "      <td>-0.1554</td>\n",
       "      <td>0.2677</td>\n",
       "      <td>-0.6813</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.4791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4418</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>-0.1862</td>\n",
       "      <td>0.4049</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>0.4666</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>-0.4205</td>\n",
       "      <td>-0.1504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>id_ffbb869f2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.0960</td>\n",
       "      <td>-1.7750</td>\n",
       "      <td>-0.3977</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>-1.3350</td>\n",
       "      <td>-0.2207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3079</td>\n",
       "      <td>-0.4473</td>\n",
       "      <td>-0.8192</td>\n",
       "      <td>0.7785</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>0.1286</td>\n",
       "      <td>-0.2618</td>\n",
       "      <td>0.5074</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>-0.0484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>id_ffd5800b6</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5174</td>\n",
       "      <td>0.2953</td>\n",
       "      <td>0.3286</td>\n",
       "      <td>-0.0428</td>\n",
       "      <td>-0.0800</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>0.5939</td>\n",
       "      <td>-0.0507</td>\n",
       "      <td>0.2811</td>\n",
       "      <td>-0.4041</td>\n",
       "      <td>-0.4948</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>-0.1356</td>\n",
       "      <td>0.5280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows Ã— 876 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05ed2ad3-a846-4882-8a34-3ac81e84ae83')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-05ed2ad3-a846-4882-8a34-3ac81e84ae83 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-05ed2ad3-a846-4882-8a34-3ac81e84ae83');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            sig_id      cp_type  cp_time cp_dose     g-0     g-1     g-2  \\\n",
       "0     id_0004d9e33       trt_cp       24      D1 -0.5458  0.1306 -0.5135   \n",
       "1     id_001897cda       trt_cp       72      D1 -0.1829  0.2320  1.2080   \n",
       "2     id_002429b5b  ctl_vehicle       24      D1  0.1852 -0.1404 -0.3911   \n",
       "3     id_00276f245       trt_cp       24      D2  0.4828  0.1955  0.3825   \n",
       "4     id_0027f1083       trt_cp       48      D1 -0.3979 -1.2680  1.9130   \n",
       "...            ...          ...      ...     ...     ...     ...     ...   \n",
       "3977  id_ff7004b87       trt_cp       24      D1  0.4571 -0.5743  3.3930   \n",
       "3978  id_ff925dd0d       trt_cp       24      D1 -0.5885 -0.2548  2.5850   \n",
       "3979  id_ffb710450       trt_cp       72      D1 -0.3985 -0.1554  0.2677   \n",
       "3980  id_ffbb869f2       trt_cp       48      D2 -1.0960 -1.7750 -0.3977   \n",
       "3981  id_ffd5800b6       trt_cp       72      D1 -0.5174  0.2953  0.3286   \n",
       "\n",
       "         g-3     g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94  \\\n",
       "0     0.4408  1.5500 -0.1644  ...  0.0981  0.7978 -0.1430 -0.2067 -0.2303   \n",
       "1    -0.4522 -0.3652 -0.3319  ... -0.1190 -0.1852 -1.0310 -1.3670 -0.3690   \n",
       "2     0.1310 -1.4380  0.2455  ... -0.2261  0.3370 -1.3840  0.8604 -1.9530   \n",
       "3     0.4244 -0.5855 -1.2020  ...  0.1260  0.1570 -0.1784 -1.1200 -0.4325   \n",
       "4     0.2057 -0.5864 -0.0166  ...  0.4965  0.7578 -0.1580  1.0510  0.5742   \n",
       "...      ...     ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "3977 -0.6202  0.8557  1.6240  ... -1.1790 -0.6422 -0.4367  0.0159 -0.6539   \n",
       "3978  0.3456  0.4401  0.3107  ...  0.0210  0.5780 -0.5888  0.8057  0.9312   \n",
       "3979 -0.6813  0.0152  0.4791  ...  0.4418  0.9153 -0.1862  0.4049  0.9568   \n",
       "3980  1.0160 -1.3350 -0.2207  ...  0.3079 -0.4473 -0.8192  0.7785  0.3133   \n",
       "3981 -0.0428 -0.0800  0.8702  ...  0.0363  0.1708  0.5939 -0.0507  0.2811   \n",
       "\n",
       "        c-95    c-96    c-97    c-98    c-99  \n",
       "0    -0.1193  0.0210 -0.0502  0.1510 -0.7750  \n",
       "1    -0.5382  0.0359 -0.4764 -1.3810 -0.7300  \n",
       "2    -1.0140  0.8662  1.0160  0.4924 -0.1942  \n",
       "3    -0.9005  0.8131 -0.1305  0.5645 -0.5809  \n",
       "4     1.0900 -0.2962 -0.5313  0.9931  1.8380  \n",
       "...      ...     ...     ...     ...     ...  \n",
       "3977 -0.4791 -1.2680 -1.1280 -0.4167 -0.6600  \n",
       "3978  1.2730  0.2614 -0.2790 -0.0131 -0.0934  \n",
       "3979  0.4666  0.0461  0.5888 -0.4205 -0.1504  \n",
       "3980  0.1286 -0.2618  0.5074  0.7430 -0.0484  \n",
       "3981 -0.4041 -0.4948  0.0757 -0.1356  0.5280  \n",
       "\n",
       "[3982 rows x 876 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features=pd.read_csv(\"/content/test_features.csv\")\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EYVo3Oq1l0Bq"
   },
   "outputs": [],
   "source": [
    "sample_submission=pd.read_csv(\"/content/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3RELaLPdiYJi"
   },
   "outputs": [],
   "source": [
    "#extract genes column names\n",
    "genes=[col for col in train_features.columns if col.startswith(\"g\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WvCwqPW8ibhK"
   },
   "outputs": [],
   "source": [
    "##extract cell column names\n",
    "cells=[col for col in train_features.columns if col.startswith(\"c\")]\n",
    "for item in ['cp_dose','cp_time','cp_type']:\n",
    "  cells.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JForJAELi5vy"
   },
   "outputs": [],
   "source": [
    "genes_train=train_features[genes]\n",
    "test_genes=test_features[genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "os3EwfWpitQ6"
   },
   "outputs": [],
   "source": [
    "#combine genes data in test and train:\n",
    "genes_data=pd.concat([genes_train,test_genes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mRCbeHapjWNg"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca_x=pca.fit_transform(genes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "_VCT-PrxjZ4X",
    "outputId": "ecdc1baa-06d8-415a-e442-8bcd40e27edd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qObmsFnLjh3y"
   },
   "source": [
    "according to above visulization, the optimum value of components to explain variance is about 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kNsE3Puqj86K"
   },
   "outputs": [],
   "source": [
    "cell_data=pd.concat([train_features[cells],test_features[cells]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "2ZAn3DfujwBK",
    "outputId": "2f76a3fa-cbcf-4ad2-c295-1a364482d701"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca_x=pca.fit_transform(cell_data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgEYO1t3kxwG"
   },
   "source": [
    "according to above visulization, the optimum value of components to explain variance is about 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3fwmcCalu9Q"
   },
   "source": [
    "# PCA features added to the original variables to give the model better representation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORB_QW9Hl1kf"
   },
   "source": [
    "1)PCA features (n=100) for genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Za-_iBEPk0wW",
    "outputId": "4b11be14-fc88-4bc9-ad35-c94d6a145748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 976)\n",
      "(3982, 976)\n"
     ]
    }
   ],
   "source": [
    "n_comp = 100\n",
    "data = pd.concat([pd.DataFrame(train_features[genes]), pd.DataFrame(test_features[genes])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data))\n",
    "train2 = data2[:train_features.shape[0]] \n",
    "test2 = data2[-test_features.shape[0]:]\n",
    "#make dataframe of these PCA arrays for test and train\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "#combine these PCA features with the main features in the primary dataset\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEsqfaIhmwX6"
   },
   "source": [
    "PCA (n=20) for cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xTjAeC81m0SI"
   },
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = 20\n",
    "data = pd.concat([pd.DataFrame(train_features[cells]), pd.DataFrame(test_features[cells])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[cells]))\n",
    "train2 = data2[:train_features.shape[0]] \n",
    "test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDU9yMe8AWnn",
    "outputId": "af4e046a-fffe-4172-89a9-b6c8ab1eb91d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 996)\n",
      "(3982, 996)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KDbw5RzAyg_"
   },
   "source": [
    "Deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNU5_dftnmZo"
   },
   "outputs": [],
   "source": [
    "#remove all low-variance features with VarianceThreshold:\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_thresh = VarianceThreshold(threshold=0.5)\n",
    "data = train_features.append(test_features)\n",
    "#not including 'sig_id','cp_type','cp_time','cp_dose'\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "#add back the features that has been removed before transformation\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yk4hoT3OuF-U"
   },
   "outputs": [],
   "source": [
    "#add the train-labels to dataset \n",
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "#control perturbation (ctrl_vehicle); control perturbations have no MoAs so we remove it\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "target = train[train_targets_scored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "noDKwnYUzRo1"
   },
   "outputs": [],
   "source": [
    "#now that we removed the ctrl_vehicle, we can remove the whole variable\n",
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2nRu70Mzm70",
    "outputId": "e43b5d6b-1443-49c5-9499-f7634a740ae1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5-alpha_reductase_inhibitor',\n",
       " '11-beta-hsd1_inhibitor',\n",
       " 'acat_inhibitor',\n",
       " 'acetylcholine_receptor_agonist',\n",
       " 'acetylcholine_receptor_antagonist',\n",
       " 'acetylcholinesterase_inhibitor',\n",
       " 'adenosine_receptor_agonist',\n",
       " 'adenosine_receptor_antagonist',\n",
       " 'adenylyl_cyclase_activator',\n",
       " 'adrenergic_receptor_agonist',\n",
       " 'adrenergic_receptor_antagonist',\n",
       " 'akt_inhibitor',\n",
       " 'aldehyde_dehydrogenase_inhibitor',\n",
       " 'alk_inhibitor',\n",
       " 'ampk_activator',\n",
       " 'analgesic',\n",
       " 'androgen_receptor_agonist',\n",
       " 'androgen_receptor_antagonist',\n",
       " 'anesthetic_-_local',\n",
       " 'angiogenesis_inhibitor',\n",
       " 'angiotensin_receptor_antagonist',\n",
       " 'anti-inflammatory',\n",
       " 'antiarrhythmic',\n",
       " 'antibiotic',\n",
       " 'anticonvulsant',\n",
       " 'antifungal',\n",
       " 'antihistamine',\n",
       " 'antimalarial',\n",
       " 'antioxidant',\n",
       " 'antiprotozoal',\n",
       " 'antiviral',\n",
       " 'apoptosis_stimulant',\n",
       " 'aromatase_inhibitor',\n",
       " 'atm_kinase_inhibitor',\n",
       " 'atp-sensitive_potassium_channel_antagonist',\n",
       " 'atp_synthase_inhibitor',\n",
       " 'atpase_inhibitor',\n",
       " 'atr_kinase_inhibitor',\n",
       " 'aurora_kinase_inhibitor',\n",
       " 'autotaxin_inhibitor',\n",
       " 'bacterial_30s_ribosomal_subunit_inhibitor',\n",
       " 'bacterial_50s_ribosomal_subunit_inhibitor',\n",
       " 'bacterial_antifolate',\n",
       " 'bacterial_cell_wall_synthesis_inhibitor',\n",
       " 'bacterial_dna_gyrase_inhibitor',\n",
       " 'bacterial_dna_inhibitor',\n",
       " 'bacterial_membrane_integrity_inhibitor',\n",
       " 'bcl_inhibitor',\n",
       " 'bcr-abl_inhibitor',\n",
       " 'benzodiazepine_receptor_agonist',\n",
       " 'beta_amyloid_inhibitor',\n",
       " 'bromodomain_inhibitor',\n",
       " 'btk_inhibitor',\n",
       " 'calcineurin_inhibitor',\n",
       " 'calcium_channel_blocker',\n",
       " 'cannabinoid_receptor_agonist',\n",
       " 'cannabinoid_receptor_antagonist',\n",
       " 'carbonic_anhydrase_inhibitor',\n",
       " 'casein_kinase_inhibitor',\n",
       " 'caspase_activator',\n",
       " 'catechol_o_methyltransferase_inhibitor',\n",
       " 'cc_chemokine_receptor_antagonist',\n",
       " 'cck_receptor_antagonist',\n",
       " 'cdk_inhibitor',\n",
       " 'chelating_agent',\n",
       " 'chk_inhibitor',\n",
       " 'chloride_channel_blocker',\n",
       " 'cholesterol_inhibitor',\n",
       " 'cholinergic_receptor_antagonist',\n",
       " 'coagulation_factor_inhibitor',\n",
       " 'corticosteroid_agonist',\n",
       " 'cyclooxygenase_inhibitor',\n",
       " 'cytochrome_p450_inhibitor',\n",
       " 'dihydrofolate_reductase_inhibitor',\n",
       " 'dipeptidyl_peptidase_inhibitor',\n",
       " 'diuretic',\n",
       " 'dna_alkylating_agent',\n",
       " 'dna_inhibitor',\n",
       " 'dopamine_receptor_agonist',\n",
       " 'dopamine_receptor_antagonist',\n",
       " 'egfr_inhibitor',\n",
       " 'elastase_inhibitor',\n",
       " 'erbb2_inhibitor',\n",
       " 'estrogen_receptor_agonist',\n",
       " 'estrogen_receptor_antagonist',\n",
       " 'faah_inhibitor',\n",
       " 'farnesyltransferase_inhibitor',\n",
       " 'fatty_acid_receptor_agonist',\n",
       " 'fgfr_inhibitor',\n",
       " 'flt3_inhibitor',\n",
       " 'focal_adhesion_kinase_inhibitor',\n",
       " 'free_radical_scavenger',\n",
       " 'fungal_squalene_epoxidase_inhibitor',\n",
       " 'gaba_receptor_agonist',\n",
       " 'gaba_receptor_antagonist',\n",
       " 'gamma_secretase_inhibitor',\n",
       " 'glucocorticoid_receptor_agonist',\n",
       " 'glutamate_inhibitor',\n",
       " 'glutamate_receptor_agonist',\n",
       " 'glutamate_receptor_antagonist',\n",
       " 'gonadotropin_receptor_agonist',\n",
       " 'gsk_inhibitor',\n",
       " 'hcv_inhibitor',\n",
       " 'hdac_inhibitor',\n",
       " 'histamine_receptor_agonist',\n",
       " 'histamine_receptor_antagonist',\n",
       " 'histone_lysine_demethylase_inhibitor',\n",
       " 'histone_lysine_methyltransferase_inhibitor',\n",
       " 'hiv_inhibitor',\n",
       " 'hmgcr_inhibitor',\n",
       " 'hsp_inhibitor',\n",
       " 'igf-1_inhibitor',\n",
       " 'ikk_inhibitor',\n",
       " 'imidazoline_receptor_agonist',\n",
       " 'immunosuppressant',\n",
       " 'insulin_secretagogue',\n",
       " 'insulin_sensitizer',\n",
       " 'integrin_inhibitor',\n",
       " 'jak_inhibitor',\n",
       " 'kit_inhibitor',\n",
       " 'laxative',\n",
       " 'leukotriene_inhibitor',\n",
       " 'leukotriene_receptor_antagonist',\n",
       " 'lipase_inhibitor',\n",
       " 'lipoxygenase_inhibitor',\n",
       " 'lxr_agonist',\n",
       " 'mdm_inhibitor',\n",
       " 'mek_inhibitor',\n",
       " 'membrane_integrity_inhibitor',\n",
       " 'mineralocorticoid_receptor_antagonist',\n",
       " 'monoacylglycerol_lipase_inhibitor',\n",
       " 'monoamine_oxidase_inhibitor',\n",
       " 'monopolar_spindle_1_kinase_inhibitor',\n",
       " 'mtor_inhibitor',\n",
       " 'mucolytic_agent',\n",
       " 'neuropeptide_receptor_antagonist',\n",
       " 'nfkb_inhibitor',\n",
       " 'nicotinic_receptor_agonist',\n",
       " 'nitric_oxide_donor',\n",
       " 'nitric_oxide_production_inhibitor',\n",
       " 'nitric_oxide_synthase_inhibitor',\n",
       " 'norepinephrine_reuptake_inhibitor',\n",
       " 'nrf2_activator',\n",
       " 'opioid_receptor_agonist',\n",
       " 'opioid_receptor_antagonist',\n",
       " 'orexin_receptor_antagonist',\n",
       " 'p38_mapk_inhibitor',\n",
       " 'p-glycoprotein_inhibitor',\n",
       " 'parp_inhibitor',\n",
       " 'pdgfr_inhibitor',\n",
       " 'pdk_inhibitor',\n",
       " 'phosphodiesterase_inhibitor',\n",
       " 'phospholipase_inhibitor',\n",
       " 'pi3k_inhibitor',\n",
       " 'pkc_inhibitor',\n",
       " 'potassium_channel_activator',\n",
       " 'potassium_channel_antagonist',\n",
       " 'ppar_receptor_agonist',\n",
       " 'ppar_receptor_antagonist',\n",
       " 'progesterone_receptor_agonist',\n",
       " 'progesterone_receptor_antagonist',\n",
       " 'prostaglandin_inhibitor',\n",
       " 'prostanoid_receptor_antagonist',\n",
       " 'proteasome_inhibitor',\n",
       " 'protein_kinase_inhibitor',\n",
       " 'protein_phosphatase_inhibitor',\n",
       " 'protein_synthesis_inhibitor',\n",
       " 'protein_tyrosine_kinase_inhibitor',\n",
       " 'radiopaque_medium',\n",
       " 'raf_inhibitor',\n",
       " 'ras_gtpase_inhibitor',\n",
       " 'retinoid_receptor_agonist',\n",
       " 'retinoid_receptor_antagonist',\n",
       " 'rho_associated_kinase_inhibitor',\n",
       " 'ribonucleoside_reductase_inhibitor',\n",
       " 'rna_polymerase_inhibitor',\n",
       " 'serotonin_receptor_agonist',\n",
       " 'serotonin_receptor_antagonist',\n",
       " 'serotonin_reuptake_inhibitor',\n",
       " 'sigma_receptor_agonist',\n",
       " 'sigma_receptor_antagonist',\n",
       " 'smoothened_receptor_antagonist',\n",
       " 'sodium_channel_inhibitor',\n",
       " 'sphingosine_receptor_agonist',\n",
       " 'src_inhibitor',\n",
       " 'steroid',\n",
       " 'syk_inhibitor',\n",
       " 'tachykinin_antagonist',\n",
       " 'tgf-beta_receptor_inhibitor',\n",
       " 'thrombin_inhibitor',\n",
       " 'thymidylate_synthase_inhibitor',\n",
       " 'tlr_agonist',\n",
       " 'tlr_antagonist',\n",
       " 'tnf_inhibitor',\n",
       " 'topoisomerase_inhibitor',\n",
       " 'transient_receptor_potential_channel_antagonist',\n",
       " 'tropomyosin_receptor_kinase_inhibitor',\n",
       " 'trpv_agonist',\n",
       " 'trpv_antagonist',\n",
       " 'tubulin_inhibitor',\n",
       " 'tyrosine_kinase_inhibitor',\n",
       " 'ubiquitin_specific_protease_inhibitor',\n",
       " 'vegfr_inhibitor',\n",
       " 'vitamin_b',\n",
       " 'vitamin_d_receptor_agonist',\n",
       " 'wnt_inhibitor']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "I9HsOfZB2mqY"
   },
   "outputs": [],
   "source": [
    "#link to multilablestratified kfold: \n",
    "#\"https://github.com/trent-b/iterative-stratification/blob/master/iterstrat/ml_stratifiers.py\"\n",
    "import iterstrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Juqh3z0j0VaN"
   },
   "outputs": [],
   "source": [
    "from iterstrat import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nK-Jc9n9z6Qw"
   },
   "outputs": [],
   "source": [
    "folds = train.copy()\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "\n",
    "#get validation and train indices and put the fold number as validation\n",
    "for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "    folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "folds['kfold'] = folds['kfold'].astype(int)\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOiDFKXu3WHX",
    "outputId": "f8e14fbd-fc61-41df-c56d-5e8a63ef90cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1201)\n",
      "(21948, 1202)\n",
      "(3624, 995)\n",
      "(21948, 207)\n"
     ]
    }
   ],
   "source": [
    "#sanity check \n",
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "n2xUc8xt3jyP"
   },
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "gXiCnYPd3rlm"
   },
   "outputs": [],
   "source": [
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "41B2Xk7m4r0B"
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer,scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs= data['x'].to(device) \n",
    "        targets= data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "    epoch_loss /= len(dataloader)\n",
    "    \n",
    "    return epoch_loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Ln7Qwtjt5flT"
   },
   "outputs": [],
   "source": [
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    epoch_loss_val = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device) \n",
    "        targets = data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        epoch_loss_val += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    epoch_loss_val /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    return epoch_loss_val, valid_preds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "mwer_IZb9Gl4"
   },
   "outputs": [],
   "source": [
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "cTuSbjxdrMAT"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(num_features, hidden_size)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.dense2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.dense3 = nn.Linear(hidden_size, num_targets)\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "A_3OMG9ArqIM"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "8I2zXzNl9fq6"
   },
   "outputs": [],
   "source": [
    "#Convert categorical variable into dummy variables.\n",
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_Km0dlE9cZ8",
    "outputId": "c51e718a-9b51-4150-8930-7ce0824bdc94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "lhCKd3NmK0b1"
   },
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "num_fold = 5\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "hidden_size=1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "c7-vkHkiK5z8"
   },
   "outputs": [],
   "source": [
    "def RUN(fold, seed):\n",
    "    seed_everything(seed)\n",
    "    #get the dummy variables for categorical data ('cp_time','cp_dose')\n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    #in every fold we make the fold number our validation and the rest for training\n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "    #make data loader \n",
    "    train_dataset = TrainDataset(x_train, y_train)\n",
    "    valid_dataset = TrainDataset(x_valid, y_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    #initilize the model\n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    ) \n",
    "    model.apply(initialize_weights)\n",
    "    model.to(DEVICE)\n",
    "    #initilize optimizer and cyclic schedular and since we don \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))  \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    #make an empy target based on the output size for train data\n",
    "    target_ = np.zeros((len(train), len(target_cols)))\n",
    "    best_loss = np.inf \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n",
    "        print(f\"Fold: {fold}, Epoch: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"Fold: {fold}, Epoch: {epoch}, valid_loss: {valid_loss}\")\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            #save the validation data predicted values\n",
    "            target_[val_idx] = valid_preds \n",
    "            #save the best predictions for validation data\n",
    "            torch.save(model.state_dict(), f\"Fold{fold}_.pth\")\n",
    "\n",
    "    #inference\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"Fold{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    return target_, predictions                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "uiQHd5docgZ2"
   },
   "outputs": [],
   "source": [
    "def kfold_training(num_fold, seed):\n",
    "    target = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    for fold in range(num_fold):\n",
    "      #run the training validation and inference for every fold\n",
    "        target_, pred  = RUN(fold, seed) \n",
    "        #we will use the mean value of all folds inference for prediction\n",
    "        predictions += pred / num_fold\n",
    "        #for validation we will use the predicted value for best loss\n",
    "        target += target_\n",
    "        \n",
    "    return target, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PAqeR9MgJB9",
    "outputId": "8d89c20a-9c4c-4c3c-fd85-394e23f4f6fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0, Epoch: 0, train_loss: 0.5211988493692183\n",
      "Fold: 0, Epoch: 0, valid_loss: 0.021259979477950504\n",
      "Fold: 0, Epoch: 1, train_loss: 0.021508109404880932\n",
      "Fold: 0, Epoch: 1, valid_loss: 0.019046979158052375\n",
      "Fold: 0, Epoch: 2, train_loss: 0.021113078784791454\n",
      "Fold: 0, Epoch: 2, valid_loss: 0.019700124194579467\n",
      "Fold: 0, Epoch: 3, train_loss: 0.01913517199969594\n",
      "Fold: 0, Epoch: 3, valid_loss: 0.018069404629724367\n",
      "Fold: 0, Epoch: 4, train_loss: 0.018269851824025744\n",
      "Fold: 0, Epoch: 4, valid_loss: 0.01718283894338778\n",
      "Fold: 0, Epoch: 5, train_loss: 0.0177567138782014\n",
      "Fold: 0, Epoch: 5, valid_loss: 0.01702781825193337\n",
      "Fold: 0, Epoch: 6, train_loss: 0.01715407158801521\n",
      "Fold: 0, Epoch: 6, valid_loss: 0.01689253377595118\n",
      "Fold: 0, Epoch: 7, train_loss: 0.016858343743597685\n",
      "Fold: 0, Epoch: 7, valid_loss: 0.01695745371814285\n",
      "Fold: 0, Epoch: 8, train_loss: 0.016924677700128243\n",
      "Fold: 0, Epoch: 8, valid_loss: 0.016536477979804787\n",
      "Fold: 0, Epoch: 9, train_loss: 0.016618269259461027\n",
      "Fold: 0, Epoch: 9, valid_loss: 0.016675111864294323\n",
      "Fold: 0, Epoch: 10, train_loss: 0.016309818300162104\n",
      "Fold: 0, Epoch: 10, valid_loss: 0.01667077126247542\n",
      "Fold: 0, Epoch: 11, train_loss: 0.016163462435529716\n",
      "Fold: 0, Epoch: 11, valid_loss: 0.016254189583872047\n",
      "Fold: 0, Epoch: 12, train_loss: 0.015895091415639374\n",
      "Fold: 0, Epoch: 12, valid_loss: 0.016270258676792895\n",
      "Fold: 0, Epoch: 13, train_loss: 0.015549064222453297\n",
      "Fold: 0, Epoch: 13, valid_loss: 0.016129283261086258\n",
      "Fold: 0, Epoch: 14, train_loss: 0.015114257125642853\n",
      "Fold: 0, Epoch: 14, valid_loss: 0.016048454732767174\n",
      "Fold: 0, Epoch: 15, train_loss: 0.014457572130081446\n",
      "Fold: 0, Epoch: 15, valid_loss: 0.016003074124455452\n",
      "Fold: 0, Epoch: 16, train_loss: 0.013587916580339273\n",
      "Fold: 0, Epoch: 16, valid_loss: 0.015910586980836732\n",
      "Fold: 0, Epoch: 17, train_loss: 0.01263737297900345\n",
      "Fold: 0, Epoch: 17, valid_loss: 0.016003570519387722\n",
      "Fold: 0, Epoch: 18, train_loss: 0.011642684890092283\n",
      "Fold: 0, Epoch: 18, valid_loss: 0.01598466004111937\n",
      "Fold: 0, Epoch: 19, train_loss: 0.01110574838610879\n",
      "Fold: 0, Epoch: 19, valid_loss: 0.01598371214100293\n",
      "Fold: 1, Epoch: 0, train_loss: 0.5189064036163947\n",
      "Fold: 1, Epoch: 0, valid_loss: 0.023778355387704713\n",
      "Fold: 1, Epoch: 1, train_loss: 0.02178567589895017\n",
      "Fold: 1, Epoch: 1, valid_loss: 0.025493465470416206\n",
      "Fold: 1, Epoch: 2, train_loss: 0.022083512939296772\n",
      "Fold: 1, Epoch: 2, valid_loss: 0.018905125877686908\n",
      "Fold: 1, Epoch: 3, train_loss: 0.019190638919995315\n",
      "Fold: 1, Epoch: 3, valid_loss: 0.017965037854654448\n",
      "Fold: 1, Epoch: 4, train_loss: 0.018213642329193543\n",
      "Fold: 1, Epoch: 4, valid_loss: 0.018308680132031442\n",
      "Fold: 1, Epoch: 5, train_loss: 0.0179953014653554\n",
      "Fold: 1, Epoch: 5, valid_loss: 0.01758381413029773\n",
      "Fold: 1, Epoch: 6, train_loss: 0.01740262126955001\n",
      "Fold: 1, Epoch: 6, valid_loss: 0.017402664412345205\n",
      "Fold: 1, Epoch: 7, train_loss: 0.016948465548077787\n",
      "Fold: 1, Epoch: 7, valid_loss: 0.017079526531909195\n",
      "Fold: 1, Epoch: 8, train_loss: 0.016706884966866262\n",
      "Fold: 1, Epoch: 8, valid_loss: 0.016941293781357152\n",
      "Fold: 1, Epoch: 9, train_loss: 0.016603122998460913\n",
      "Fold: 1, Epoch: 9, valid_loss: 0.01691580880433321\n",
      "Fold: 1, Epoch: 10, train_loss: 0.01638522382447685\n",
      "Fold: 1, Epoch: 10, valid_loss: 0.016831904756171363\n",
      "Fold: 1, Epoch: 11, train_loss: 0.016210960622444964\n",
      "Fold: 1, Epoch: 11, valid_loss: 0.016613812797835897\n",
      "Fold: 1, Epoch: 12, train_loss: 0.015912909053967916\n",
      "Fold: 1, Epoch: 12, valid_loss: 0.016502102224954535\n",
      "Fold: 1, Epoch: 13, train_loss: 0.015591807521717705\n",
      "Fold: 1, Epoch: 13, valid_loss: 0.016405580937862395\n",
      "Fold: 1, Epoch: 14, train_loss: 0.015097122895868792\n",
      "Fold: 1, Epoch: 14, valid_loss: 0.01631579997816256\n",
      "Fold: 1, Epoch: 15, train_loss: 0.014471146244339752\n",
      "Fold: 1, Epoch: 15, valid_loss: 0.016285750615809644\n",
      "Fold: 1, Epoch: 16, train_loss: 0.01369019720595384\n",
      "Fold: 1, Epoch: 16, valid_loss: 0.01624891869723797\n",
      "Fold: 1, Epoch: 17, train_loss: 0.012669101195490879\n",
      "Fold: 1, Epoch: 17, valid_loss: 0.016260065085121563\n",
      "Fold: 1, Epoch: 18, train_loss: 0.0117414395486855\n",
      "Fold: 1, Epoch: 18, valid_loss: 0.01627769587295396\n",
      "Fold: 1, Epoch: 19, train_loss: 0.011231749736960384\n",
      "Fold: 1, Epoch: 19, valid_loss: 0.01631861374314342\n",
      "Fold: 2, Epoch: 0, train_loss: 0.5203517857210144\n",
      "Fold: 2, Epoch: 0, valid_loss: 0.027121913539511818\n",
      "Fold: 2, Epoch: 1, train_loss: 0.022324067365000214\n",
      "Fold: 2, Epoch: 1, valid_loss: 0.05121296282325472\n",
      "Fold: 2, Epoch: 2, train_loss: 0.026163514912722334\n",
      "Fold: 2, Epoch: 2, valid_loss: 0.01904396465314286\n",
      "Fold: 2, Epoch: 3, train_loss: 0.019930867991153744\n",
      "Fold: 2, Epoch: 3, valid_loss: 0.018135039135813713\n",
      "Fold: 2, Epoch: 4, train_loss: 0.019309556320903525\n",
      "Fold: 2, Epoch: 4, valid_loss: 0.017752327849822384\n",
      "Fold: 2, Epoch: 5, train_loss: 0.018225745837865532\n",
      "Fold: 2, Epoch: 5, valid_loss: 0.01734523291566542\n",
      "Fold: 2, Epoch: 6, train_loss: 0.01769463743582584\n",
      "Fold: 2, Epoch: 6, valid_loss: 0.017186297423073222\n",
      "Fold: 2, Epoch: 7, train_loss: 0.017273572928177706\n",
      "Fold: 2, Epoch: 7, valid_loss: 0.016992963903716633\n",
      "Fold: 2, Epoch: 8, train_loss: 0.016973004096012184\n",
      "Fold: 2, Epoch: 8, valid_loss: 0.017001310549676418\n",
      "Fold: 2, Epoch: 9, train_loss: 0.016693419614887756\n",
      "Fold: 2, Epoch: 9, valid_loss: 0.01669824155313628\n",
      "Fold: 2, Epoch: 10, train_loss: 0.01653532350462848\n",
      "Fold: 2, Epoch: 10, valid_loss: 0.016608307537223612\n",
      "Fold: 2, Epoch: 11, train_loss: 0.016192915059788072\n",
      "Fold: 2, Epoch: 11, valid_loss: 0.016572573568139758\n",
      "Fold: 2, Epoch: 12, train_loss: 0.01586120217309698\n",
      "Fold: 2, Epoch: 12, valid_loss: 0.0163567594651665\n",
      "Fold: 2, Epoch: 13, train_loss: 0.015505703402332205\n",
      "Fold: 2, Epoch: 13, valid_loss: 0.016236176235335215\n",
      "Fold: 2, Epoch: 14, train_loss: 0.014988670361808676\n",
      "Fold: 2, Epoch: 14, valid_loss: 0.01615831817367247\n",
      "Fold: 2, Epoch: 15, train_loss: 0.014372111952769152\n",
      "Fold: 2, Epoch: 15, valid_loss: 0.016178207897714205\n",
      "Fold: 2, Epoch: 16, train_loss: 0.013568589450332565\n",
      "Fold: 2, Epoch: 16, valid_loss: 0.016179295043860162\n",
      "Fold: 2, Epoch: 17, train_loss: 0.012473814466131338\n",
      "Fold: 2, Epoch: 17, valid_loss: 0.01625046203179019\n",
      "Fold: 2, Epoch: 18, train_loss: 0.011564877660324177\n",
      "Fold: 2, Epoch: 18, valid_loss: 0.01623817096863474\n",
      "Fold: 2, Epoch: 19, train_loss: 0.011011130135992298\n",
      "Fold: 2, Epoch: 19, valid_loss: 0.016246467935187477\n",
      "Fold: 3, Epoch: 0, train_loss: 0.5212852481239731\n",
      "Fold: 3, Epoch: 0, valid_loss: 0.02164634322481496\n",
      "Fold: 3, Epoch: 1, train_loss: 0.02660826909477296\n",
      "Fold: 3, Epoch: 1, valid_loss: 0.3131326594523021\n",
      "Fold: 3, Epoch: 2, train_loss: 0.027774063235931637\n",
      "Fold: 3, Epoch: 2, valid_loss: 0.025352888607553074\n",
      "Fold: 3, Epoch: 3, train_loss: 0.021527916085028995\n",
      "Fold: 3, Epoch: 3, valid_loss: 0.019926076542053903\n",
      "Fold: 3, Epoch: 4, train_loss: 0.01990230462473372\n",
      "Fold: 3, Epoch: 4, valid_loss: 0.018687262279646736\n",
      "Fold: 3, Epoch: 5, train_loss: 0.01883605333800981\n",
      "Fold: 3, Epoch: 5, valid_loss: 0.018627314695290158\n",
      "Fold: 3, Epoch: 6, train_loss: 0.01848842382458025\n",
      "Fold: 3, Epoch: 6, valid_loss: 0.018253763259521553\n",
      "Fold: 3, Epoch: 7, train_loss: 0.018065742473455444\n",
      "Fold: 3, Epoch: 7, valid_loss: 0.017742511230920043\n",
      "Fold: 3, Epoch: 8, train_loss: 0.017666168207221705\n",
      "Fold: 3, Epoch: 8, valid_loss: 0.017702186347118447\n",
      "Fold: 3, Epoch: 9, train_loss: 0.017195030567708654\n",
      "Fold: 3, Epoch: 9, valid_loss: 0.017195234314671584\n",
      "Fold: 3, Epoch: 10, train_loss: 0.01685839578293372\n",
      "Fold: 3, Epoch: 10, valid_loss: 0.017235167963164193\n",
      "Fold: 3, Epoch: 11, train_loss: 0.016647582617250904\n",
      "Fold: 3, Epoch: 11, valid_loss: 0.01705867395337139\n",
      "Fold: 3, Epoch: 12, train_loss: 0.016336749395544546\n",
      "Fold: 3, Epoch: 12, valid_loss: 0.01682563392179353\n",
      "Fold: 3, Epoch: 13, train_loss: 0.015927120746261833\n",
      "Fold: 3, Epoch: 13, valid_loss: 0.016614250279963015\n",
      "Fold: 3, Epoch: 14, train_loss: 0.015602591737726892\n",
      "Fold: 3, Epoch: 14, valid_loss: 0.01652818029480321\n",
      "Fold: 3, Epoch: 15, train_loss: 0.015162260386336973\n",
      "Fold: 3, Epoch: 15, valid_loss: 0.016394005582800932\n",
      "Fold: 3, Epoch: 16, train_loss: 0.014543220340071814\n",
      "Fold: 3, Epoch: 16, valid_loss: 0.01629630138299295\n",
      "Fold: 3, Epoch: 17, train_loss: 0.013985995439461607\n",
      "Fold: 3, Epoch: 17, valid_loss: 0.016156018525362015\n",
      "Fold: 3, Epoch: 18, train_loss: 0.013463885455891706\n",
      "Fold: 3, Epoch: 18, valid_loss: 0.016185834551496164\n",
      "Fold: 3, Epoch: 19, train_loss: 0.013179512667483177\n",
      "Fold: 3, Epoch: 19, valid_loss: 0.0161545833040561\n",
      "Fold: 4, Epoch: 0, train_loss: 0.521144834401059\n",
      "Fold: 4, Epoch: 0, valid_loss: 0.023123722949198313\n",
      "Fold: 4, Epoch: 1, train_loss: 0.02159982721955664\n",
      "Fold: 4, Epoch: 1, valid_loss: 0.019494360419256347\n",
      "Fold: 4, Epoch: 2, train_loss: 0.020393345727706732\n",
      "Fold: 4, Epoch: 2, valid_loss: 0.018479605710932188\n",
      "Fold: 4, Epoch: 3, train_loss: 0.01967885931008968\n",
      "Fold: 4, Epoch: 3, valid_loss: 0.018983035268528122\n",
      "Fold: 4, Epoch: 4, train_loss: 0.01856112232485759\n",
      "Fold: 4, Epoch: 4, valid_loss: 0.017249366880527566\n",
      "Fold: 4, Epoch: 5, train_loss: 0.01739180147431899\n",
      "Fold: 4, Epoch: 5, valid_loss: 0.017960808133440358\n",
      "Fold: 4, Epoch: 6, train_loss: 0.01740641817720472\n",
      "Fold: 4, Epoch: 6, valid_loss: 0.017076280393770762\n",
      "Fold: 4, Epoch: 7, train_loss: 0.016925616156093885\n",
      "Fold: 4, Epoch: 7, valid_loss: 0.016732138608183178\n",
      "Fold: 4, Epoch: 8, train_loss: 0.016784234188389088\n",
      "Fold: 4, Epoch: 8, valid_loss: 0.01667062161224229\n",
      "Fold: 4, Epoch: 9, train_loss: 0.01658090290384016\n",
      "Fold: 4, Epoch: 9, valid_loss: 0.016578133563910212\n",
      "Fold: 4, Epoch: 10, train_loss: 0.01635263118541975\n",
      "Fold: 4, Epoch: 10, valid_loss: 0.016482490274522987\n",
      "Fold: 4, Epoch: 11, train_loss: 0.01618248206036894\n",
      "Fold: 4, Epoch: 11, valid_loss: 0.016372096139405455\n",
      "Fold: 4, Epoch: 12, train_loss: 0.015911441923969465\n",
      "Fold: 4, Epoch: 12, valid_loss: 0.016324607336095403\n",
      "Fold: 4, Epoch: 13, train_loss: 0.015571627752396507\n",
      "Fold: 4, Epoch: 13, valid_loss: 0.016169783792325428\n",
      "Fold: 4, Epoch: 14, train_loss: 0.015097288394589787\n",
      "Fold: 4, Epoch: 14, valid_loss: 0.016114840709737368\n",
      "Fold: 4, Epoch: 15, train_loss: 0.014483107733067827\n",
      "Fold: 4, Epoch: 15, valid_loss: 0.01602304375597409\n",
      "Fold: 4, Epoch: 16, train_loss: 0.013649426468148611\n",
      "Fold: 4, Epoch: 16, valid_loss: 0.01607699372938701\n",
      "Fold: 4, Epoch: 17, train_loss: 0.012664151654673227\n",
      "Fold: 4, Epoch: 17, valid_loss: 0.016032934987119267\n",
      "Fold: 4, Epoch: 18, train_loss: 0.011721998007725115\n",
      "Fold: 4, Epoch: 18, valid_loss: 0.016069666216416017\n",
      "Fold: 4, Epoch: 19, train_loss: 0.01116689723794875\n",
      "Fold: 4, Epoch: 19, valid_loss: 0.01606858258268663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3678: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self[col] = igetitem(value, i)\n"
     ]
    }
   ],
   "source": [
    "target2, predictions2 = kfold_training(num_fold,seed=2)\n",
    "train[target_cols] = target2\n",
    "test[target_cols] = predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ece8K65kcaW"
   },
   "source": [
    "Based on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "596F5Cl-iZpt",
    "outputId": "5bc99524-915e-41b4-b0d0-260dd61e5753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  0.01480537585492728\n"
     ]
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], \n",
    "                                                                     on='sig_id', how='left').fillna(0)\n",
    "#evaluate the \n",
    "y_true = train_targets_scored[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_col = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_col / target.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "keXusif8lvkE"
   },
   "outputs": [],
   "source": [
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOjKhBrAuYJr"
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
